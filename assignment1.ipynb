{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fdca94",
   "metadata": {},
   "source": [
    "# COMP 3610 — Assignment 1: Data Pipeline & Visualization Dashboard\n",
    "\n",
    "**Student:** Nie-l Constance |\n",
    "**Course:** COMP 3610 — Big Data Analytics |\n",
    "**Semester:** II, 2025-2026 \n",
    "\n",
    "**AI Assistance Disclosure:** This notebook used Deepseek and VSCode Autocompletions for code suggestions and structure. All work and final code are authored and reviewed by the student (myself)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe10ad",
   "metadata": {},
   "source": [
    "## Part 1: Data Ingestion\n",
    "This section downloads the required files, validates the raw data schema and datetimes, and saves raw files into `data/raw/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed34f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Programmatic download and validation\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_DIR = Path(\"data/raw\")\n",
    "TRIP_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "ZONE_URL = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "TRIP_PATH = RAW_DIR / \"yellow_tripdata_2024-01.parquet\"\n",
    "ZONE_PATH = RAW_DIR / \"taxi_zone_lookup.csv\"\n",
    "\n",
    "def ensure_raw_dir():\n",
    "    RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download_file(url, dest, max_retries=3):\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        resp = requests.get(url, timeout=30)\n",
    "        if resp.status_code == 200:\n",
    "            data = resp.content\n",
    "            # quick sanity size check\n",
    "            if len(data) < 100:\n",
    "                raise ValueError(f\"Downloaded content for {url} looks too small ({len(data)} bytes)\")\n",
    "            with open(dest, \"wb\") as f:\n",
    "                f.write(data)\n",
    "            return dest\n",
    "        if attempt == max_retries:\n",
    "            resp.raise_for_status()\n",
    "    raise RuntimeError(\"Unreachable\")\n",
    "\n",
    "def download_if_missing(url, dest):\n",
    "    if not dest.exists():\n",
    "        print(f\"Downloading {url} ...\")\n",
    "        download_file(url, dest)\n",
    "    else:\n",
    "        print(f\"Found existing file: {dest}\")\n",
    "\n",
    "# Create .gitignore entry for data/ if missing\n",
    "GITIGNORE = Path(\".gitignore\")\n",
    "if GITIGNORE.exists():\n",
    "    txt = GITIGNORE.read_text()\n",
    "    if \"data/\" not in txt:\n",
    "        with open(GITIGNORE, \"a\") as f:\n",
    "            f.write(\"\\n# Ignore raw/processed data\\ndata/\\n\")\n",
    "else:\n",
    "    with open(GITIGNORE, \"w\") as f:\n",
    "        f.write(\"# Generated by assignment\\n# Ignore raw/processed data\\ndata/\\n\")\n",
    "\n",
    "ensure_raw_dir()\n",
    "download_if_missing(TRIP_URL, TRIP_PATH)\n",
    "download_if_missing(ZONE_URL, ZONE_PATH)\n",
    "\n",
    "print(\"Download complete. Files saved to\", RAW_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9ab4c3",
   "metadata": {},
   "source": [
    "### Data validation checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation helpers\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Define expected column set per assignment spec\n",
    "EXPECTED_COLS = [\n",
    "    \"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"PULocationID\",\"DOLocationID\",\n",
    "    \"passenger_count\",\"trip_distance\",\"fare_amount\",\"tip_amount\",\"total_amount\",\"payment_type\"\n",
    "]\n",
    "\n",
    "def validate_trip_df(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Validate that the raw trip data contains all required columns and valid datetime fields.\n",
    "    Raises exceptions if validation fails per assignment requirement.\n",
    "    \"\"\"\n",
    "    # Check for missing columns\n",
    "    missing = [c for c in EXPECTED_COLS if c not in df.columns]\n",
    "    if missing:\n",
    "        error_msg = f\"VALIDATION FAILED: Missing required columns: {missing}\"\n",
    "        print(error_msg, file=sys.stderr)\n",
    "        raise AssertionError(error_msg)\n",
    "    \n",
    "    # Validate datetime columns can be parsed\n",
    "    for col in [\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\"]:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        if df[col].isna().all():\n",
    "            error_msg = f\"VALIDATION FAILED: Column '{col}' could not be parsed as datetimes - all values are NaT\"\n",
    "            print(error_msg, file=sys.stderr)\n",
    "            raise TypeError(error_msg)\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"WARNING: {nan_count:,} NaT values found in {col} (will be removed in cleaning step)\")\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            error_msg = f\"VALIDATION FAILED: Column '{col}' is not a datetime type after parsing\"\n",
    "            print(error_msg, file=sys.stderr)\n",
    "            raise TypeError(error_msg)\n",
    "    \n",
    "    # Report validation success\n",
    "    print(\"✓ VALIDATION PASSED\")\n",
    "    print(f\"✓ All {len(EXPECTED_COLS)} expected columns present\")\n",
    "    print(f\"✓ Datetime columns are valid and parseable\")\n",
    "    print(f\"\\n--- Data Summary ---\")\n",
    "    print(f\"Total rows in dataset: {len(df):,}\")\n",
    "    print(f\"\\nColumn data types:\")\n",
    "    for col in EXPECTED_COLS:\n",
    "        print(f\"  {col}: {df[col].dtype}\")\n",
    "    return True\n",
    "\n",
    "# Load full dataset and validate\n",
    "print(\"Loading dataset from:\", TRIP_PATH)\n",
    "trip_df = pd.read_parquet(TRIP_PATH)\n",
    "print(f\"Raw dataset loaded: {len(trip_df):,} rows\\n\")\n",
    "validate_trip_df(trip_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e04f2",
   "metadata": {},
   "source": [
    "## Part 2: Data Transformation & Analysis\n",
    "This section performs data cleaning and feature engineering using Pandas. Code is organized into reusable functions and tested with small examples where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset (already downloaded in Part 1)\n",
    "# Ensure datetimes are parsed\n",
    "trip_df[\"tpep_pickup_datetime\"] = pd.to_datetime(trip_df[\"tpep_pickup_datetime\"], errors=\"coerce\")\n",
    "trip_df[\"tpep_dropoff_datetime\"] = pd.to_datetime(trip_df[\"tpep_dropoff_datetime\"], errors=\"coerce\")\n",
    "\n",
    "def cleaning_summary(orig, after_null, after_invalid, after_time):\n",
    "    removed_nulls = orig - after_null\n",
    "    removed_invalid = after_null - after_invalid\n",
    "    removed_time = after_invalid - after_time\n",
    "    return {\n",
    "        \"original_rows\": orig,\n",
    "        \"removed_nulls\": removed_nulls,\n",
    "        \"removed_invalid\": removed_invalid,\n",
    "        \"removed_time\": removed_time,\n",
    "        \"final_rows\": after_time\n",
    "    }\n",
    "\n",
    "def clean_trip_df(df: pd.DataFrame):\n",
    "    # Work on a copy to avoid side effects\n",
    "    df = df.copy()\n",
    "    orig = len(df)\n",
    "    # Drop nulls in critical columns\n",
    "    df = df.dropna(subset=[\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"PULocationID\",\"DOLocationID\",\"fare_amount\"])\n",
    "    after_null = len(df)\n",
    "    # Filter out invalid trips\n",
    "    df = df[(df[\"trip_distance\"] > 0) & (df[\"fare_amount\"] >= 0) & (df[\"fare_amount\"] <= 500)]\n",
    "    after_invalid = len(df)\n",
    "    # Remove rows where dropoff is before pickup\n",
    "    df = df[df[\"tpep_dropoff_datetime\"] >= df[\"tpep_pickup_datetime\"]]\n",
    "    after_time = len(df)\n",
    "    summary = cleaning_summary(orig, after_null, after_invalid, after_time)\n",
    "\n",
    "    print(\"Cleaning summary (full dataset):\")\n",
    "    print(f\"- Original rows: {summary['original_rows']:,}\")\n",
    "    print(f\"- Removed nulls in critical columns: {summary['removed_nulls']:,}\")\n",
    "    print(f\"- Removed invalid distance/fare rows: {summary['removed_invalid']:,}\")\n",
    "    print(f\"- Removed dropoff-before-pickup rows: {summary['removed_time']:,}\")\n",
    "    print(f\"- Final rows: {summary['final_rows']:,}\")\n",
    "\n",
    "    removals = {\n",
    "        \"null critical values\": summary[\"removed_nulls\"],\n",
    "        \"invalid distance/fare\": summary[\"removed_invalid\"],\n",
    "        \"dropoff before pickup\": summary[\"removed_time\"],\n",
    "    }\n",
    "    main_reason = max(removals, key=removals.get)\n",
    "    print(f\"Observation: The largest share of removals came from {main_reason}.\")\n",
    "\n",
    "    return df, summary\n",
    "\n",
    "trip_clean, clean_summary = clean_trip_df(trip_df)\n",
    "print(\"Cleaned rows:\", len(trip_clean))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d3efe",
   "metadata": {},
   "source": [
    "### Feature engineering (exactly 4 new columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb08c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    # 1) trip_duration_minutes\n",
    "    df[\"trip_duration_minutes\"] = (df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]).dt.total_seconds() / 60.0\n",
    "    df[\"trip_duration_minutes\"] = df[\"trip_duration_minutes\"].clip(lower=0)\n",
    "    # 2) trip_speed_mph (distance / hours) - handle zero duration safely\n",
    "    duration_hours = df[\"trip_duration_minutes\"] / 60.0\n",
    "    df[\"trip_speed_mph\"] = df[\"trip_distance\"].div(duration_hours)\n",
    "    df[\"trip_speed_mph\"] = df[\"trip_speed_mph\"].replace([float(\"inf\"), -float(\"inf\")], 0).fillna(0)\n",
    "    # 3) pickup_hour (0-23)\n",
    "    df[\"pickup_hour\"] = df[\"tpep_pickup_datetime\"].dt.hour\n",
    "    # 4) pickup_day_of_week (Monday - Sunday)\n",
    "    df[\"pickup_day_of_week\"] = df[\"tpep_pickup_datetime\"].dt.day_name()\n",
    "    return df\n",
    "\n",
    "# Apply to full cleaned dataset\n",
    "trip_fe = feature_engineer(trip_clean)\n",
    "print(trip_fe[[\"trip_duration_minutes\",\"trip_speed_mph\",\"pickup_hour\",\"pickup_day_of_week\"]].head())\n",
    "\n",
    "# Simple unit test on a tiny example\n",
    "_test = pd.DataFrame([\n",
    "    {\n",
    "        \"tpep_pickup_datetime\": \"2024-01-01 10:00:00\",\n",
    "        \"tpep_dropoff_datetime\": \"2024-01-01 10:05:00\",\n",
    "        \"PULocationID\": 1,\n",
    "        \"DOLocationID\": 2,\n",
    "        \"trip_distance\": 1.2,\n",
    "        \"fare_amount\": 5.0,\n",
    "        \"tip_amount\": 1.0,\n",
    "        \"total_amount\": 6.0,\n",
    "        \"payment_type\": 1,\n",
    "    }\n",
    "])\n",
    "_test[\"tpep_pickup_datetime\"] = pd.to_datetime(_test[\"tpep_pickup_datetime\"])\n",
    "_test[\"tpep_dropoff_datetime\"] = pd.to_datetime(_test[\"tpep_dropoff_datetime\"])\n",
    "_test_fe = feature_engineer(_test)\n",
    "assert _test_fe.loc[0, \"trip_duration_minutes\"] == 5.0\n",
    "assert _test_fe.loc[0, \"pickup_hour\"] == 10\n",
    "print(\"Feature engineering tests passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119fa714",
   "metadata": {},
   "source": [
    "## Part 2: SQL Analysis using DuckDB\n",
    "We'll load the cleaned DataFrame into DuckDB (in-memory) and run the 5 required queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb9e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Register cleaned data into DuckDB for SQL queries\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "\n",
    "# Use full feature-engineered dataset\n",
    "con.register(\"trips\", trip_fe)\n",
    "\n",
    "# Load zones table\n",
    "zones_df_full = pd.read_csv(ZONE_PATH)\n",
    "if \"Zone\" in zones_df_full.columns:\n",
    "    zones_df_full = zones_df_full.rename(columns={\"Zone\": \"zone\"})\n",
    "con.register(\"zones\", zones_df_full)\n",
    "\n",
    "print(\"DuckDB in-memory tables registered: trips and zones\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9b392",
   "metadata": {},
   "source": [
    "### Query 1: Top 10 busiest pickup zones by total number of trips\n",
    "This query returns the top 10 pickup zones by trip count, including zone names from the taxi zone lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a69302",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"\"\"\n",
    "SELECT t.PULocationID, z.zone, count(*) AS trips\n",
    "FROM trips AS t\n",
    "JOIN zones AS z ON t.PULocationID = z.LocationID\n",
    "GROUP BY t.PULocationID, z.zone\n",
    "ORDER BY trips DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(\"SQL Query 1 results:\")\n",
    "q1_df = con.execute(q1).df()\n",
    "print(q1_df)\n",
    "if not q1_df.empty:\n",
    "    top_zone = q1_df.iloc[0][\"zone\"]\n",
    "    top_trips = int(q1_df.iloc[0][\"trips\"])\n",
    "    print(f\"Interpretation: The busiest pickup zone is {top_zone} with {top_trips:,} trips, indicating a strong concentration of demand in that area.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d5ed74",
   "metadata": {},
   "source": [
    "### Query 2: Average fare by hour of day\n",
    "This query computes the average fare amount for each hour (0-23) and orders results by hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7dabd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = \"\"\"\n",
    "SELECT pickup_hour, AVG(fare_amount) AS avg_fare\n",
    "FROM trips\n",
    "GROUP BY pickup_hour\n",
    "ORDER BY pickup_hour\n",
    "\"\"\"\n",
    "print(\"SQL Query 2 results:\")\n",
    "q2_df = con.execute(q2).df()\n",
    "print(q2_df)\n",
    "if not q2_df.empty:\n",
    "    peak = q2_df.loc[q2_df[\"avg_fare\"].idxmax()]\n",
    "    low = q2_df.loc[q2_df[\"avg_fare\"].idxmin()]\n",
    "    print(f\"Interpretation: The highest average fare occurs around {int(peak['pickup_hour'])}:00 (${peak['avg_fare']:.2f}), while the lowest is around {int(low['pickup_hour'])}:00 (${low['avg_fare']:.2f}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41691264",
   "metadata": {},
   "source": [
    "### Query 3: Payment type percentage\n",
    "This query computes the count and percentage share of trips by payment type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3acf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = \"\"\"\n",
    "SELECT payment_type, COUNT(*) AS cnt, 100.0 * COUNT(*) / SUM(COUNT(*)) OVER() AS pct\n",
    "FROM trips\n",
    "GROUP BY payment_type\n",
    "ORDER BY cnt DESC\n",
    "\"\"\"\n",
    "print(\"SQL Query 3 results:\")\n",
    "q3_df = con.execute(q3).df()\n",
    "print(q3_df)\n",
    "if not q3_df.empty:\n",
    "    pay_map = {1: \"Credit card\", 2: \"Cash\", 3: \"No charge\", 4: \"Dispute\", 5: \"Unknown\"}\n",
    "    top_code = int(q3_df.iloc[0][\"payment_type\"])\n",
    "    top_label = pay_map.get(top_code, f\"Code {top_code}\")\n",
    "    top_pct = q3_df.iloc[0][\"pct\"]\n",
    "    print(f\"Interpretation: {top_label} is the dominant payment method at {top_pct:.1f}% of trips.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa9937",
   "metadata": {},
   "source": [
    "### Query 4: Average tip percentage by day (credit card payments only)\n",
    "This query computes the average tip percentage (tip_amount / fare_amount) by day of week, restricted to credit card payments (payment_type = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f5c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q4 = \"\"\"\n",
    "SELECT pickup_day_of_week, AVG(CASE WHEN fare_amount > 0 THEN tip_amount / fare_amount ELSE NULL END) AS avg_tip_pct\n",
    "FROM trips\n",
    "WHERE payment_type = 1\n",
    "GROUP BY pickup_day_of_week\n",
    "ORDER BY array_position(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'], pickup_day_of_week)\n",
    "\"\"\"\n",
    "print(\"SQL Query 4 results:\")\n",
    "q4_df = con.execute(q4).df()\n",
    "print(q4_df)\n",
    "if not q4_df.empty:\n",
    "    peak = q4_df.loc[q4_df[\"avg_tip_pct\"].idxmax()]\n",
    "    print(f\"Interpretation: The highest average credit-card tip percentage occurs on {peak['pickup_day_of_week']} ({peak['avg_tip_pct']:.2%}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148a8241",
   "metadata": {},
   "source": [
    "### Query 5: Top 5 pickup-dropoff zone pairs\n",
    "This query finds the top 5 most common pickup-dropoff zone pairs and includes zone names for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b1cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "q5 = \"\"\"\n",
    "SELECT t.PULocationID AS pu_ID, p.zone AS pu_zone, t.DOLocationID AS do_ID, d.zone AS do_zone, COUNT(*) AS trips\n",
    "FROM trips t\n",
    "LEFT JOIN zones p ON t.PULocationID = p.LocationID\n",
    "LEFT JOIN zones d ON t.DOLocationID = d.LocationID\n",
    "GROUP BY pu_ID, pu_zone, do_ID, do_zone\n",
    "ORDER BY trips DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "print(\"SQL Query 5 results:\")\n",
    "q5_df = con.execute(q5).df()\n",
    "print(q5_df)\n",
    "if not q5_df.empty:\n",
    "    top_pair = q5_df.iloc[0]\n",
    "    print(f\"Interpretation: The most common pickup-dropoff pair is {top_pair['pu_zone']} to {top_pair['do_zone']} with {int(top_pair['trips']):,} trips.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c185be",
   "metadata": {},
   "source": [
    "## Part 3: Dashboard Development (prototypes)\n",
    "This section prototypes the visualizations required for the Streamlit app using Plotly. Each plot is followed by a short interpretation (2–3 sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8024236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Use full feature-engineered dataset for prototypes\n",
    "# If you want to speed up plotting, uncomment the sample line below\n",
    "# df_plot = trip_fe.sample(n=min(len(trip_fe), 200000), random_state=42)\n",
    "\n",
    "def center_titles(fig, bold=False):\n",
    "    fig.update_layout(title_x=0.5)\n",
    "    return fig\n",
    "\n",
    "df_plot = trip_fe.copy()\n",
    "\n",
    "# Add zone names using lookup table\n",
    "zones_df_full = pd.read_csv(ZONE_PATH)\n",
    "if \"Zone\" in zones_df_full.columns:\n",
    "    zones_df_full = zones_df_full.rename(columns={\"Zone\": \"zone\"})\n",
    "zone_map = zones_df_full.set_index(\"LocationID\")[\"zone\"].to_dict()\n",
    "\n",
    "df_plot[\"PU_zone\"] = df_plot[\"PULocationID\"].map(zone_map)\n",
    "df_plot[\"DO_zone\"] = df_plot[\"DOLocationID\"].map(zone_map)\n",
    "\n",
    "# Visualization 1: Top 10 pickup zones (bar)\n",
    "top_pu = df_plot[\"PU_zone\"].value_counts().head(10).reset_index()\n",
    "top_pu.columns = [\"zone\",\"trips\"]\n",
    "fig_bar = center_titles(\n",
    "    px.bar(\n",
    "        top_pu, \n",
    "        x=\"zone\", \n",
    "        y=\"trips\", \n",
    "        title=\"Top Pickup Zones\",\n",
    "        labels={\"zone\": \"Pickup Zone\", \"trips\": \"Number of Trips\"},\n",
    "    )\n",
    ")\n",
    "fig_bar.show()\n",
    "if not top_pu.empty:\n",
    "    top_zone = top_pu.loc[0, \"zone\"]\n",
    "    top_trips = int(top_pu.loc[0, \"trips\"])\n",
    "    top_share = 100.0 * top_trips / len(df_plot)\n",
    "    print(f\"Interpretation: The busiest pickup zone is {top_zone} with {top_trips:,} trips ({top_share:.1f}% of all trips). The top 10 zones together account for {100.0 * top_pu['trips'].sum() / len(df_plot):.1f}% of trips, showing strong spatial concentration.\")\n",
    "\n",
    "# Visualization 2: Avg fare by hour (line)\n",
    "hourly = df_plot.groupby(\"pickup_hour\")[\"fare_amount\"].mean().reset_index()\n",
    "fig_line = center_titles(\n",
    "    px.line(\n",
    "        hourly, \n",
    "        x=\"pickup_hour\", \n",
    "        y=\"fare_amount\", \n",
    "        title=\"Average Fare by Hour\", \n",
    "        markers=True,\n",
    "        labels={\"pickup_hour\": \"Pickup Hour\", \"fare_amount\": \"Average Fare ($)\"},\n",
    "    )\n",
    ")\n",
    "fig_line.update_xaxes(range=[0, 23]) # Set x-axis range to cover all hours\n",
    "fig_line.update_xaxes(dtick=1) # Show every hour on x-axis\n",
    "fig_line.show()\n",
    "if not hourly.empty:\n",
    "    peak = hourly.loc[hourly[\"fare_amount\"].idxmax()]\n",
    "    low = hourly.loc[hourly[\"fare_amount\"].idxmin()]\n",
    "    print(f\"Interpretation: The highest average fare occurs around {int(peak['pickup_hour'])}:00 (${peak['fare_amount']:.2f}), while the lowest is around {int(low['pickup_hour'])}:00 (${low['fare_amount']:.2f}). This indicates clear hourly pricing patterns likely tied to demand peaks.\")\n",
    "\n",
    "# Visualization 3: Histogram of trip distances\n",
    "fig_hist = center_titles(\n",
    "    px.histogram(\n",
    "        df_plot, \n",
    "        x=\"trip_distance\", \n",
    "        nbins=30, \n",
    "        title=\"Trip Distance Distribution\",\n",
    "        labels={\"trip_distance\": \"Trip Distance (miles)\"}\n",
    "    )\n",
    ")\n",
    "fig_hist.show()\n",
    "median_dist = df_plot[\"trip_distance\"].median()\n",
    "percentile_90 = df_plot[\"trip_distance\"].quantile(0.9)\n",
    "print(f\"Interpretation: The median trip distance is {median_dist:.2f} miles, while 90% of trips are under {percentile_90:.2f} miles. This shows most trips are short with a long tail of longer journeys.\")\n",
    "\n",
    "# Visualization 4: Payment type breakdown (Pie Chart)\n",
    "pay_map = {1:\"Credit card\",2:\"Cash\",3:\"No charge\",4:\"Dispute\",5:\"Unknown\"}\n",
    "df_plot[\"payment_label\"] = df_plot[\"payment_type\"].map(pay_map).fillna(\"Other\")\n",
    "fig_pie = center_titles(\n",
    "    px.pie(\n",
    "        df_plot, \n",
    "        names=\"payment_label\", \n",
    "        title=\"Payment Types\",\n",
    "        labels={\"payment_label\": \"Payment Type\"}\n",
    "    )\n",
    ")\n",
    "fig_pie.show()\n",
    "pay_counts = df_plot[\"payment_label\"].value_counts(normalize=True) * 100\n",
    "if not pay_counts.empty:\n",
    "    top_pay = pay_counts.index[0]\n",
    "    top_pct = pay_counts.iloc[0]\n",
    "    print(f\"Interpretation: {top_pay} is the dominant payment method at {top_pct:.1f}% of trips. Payment mix influences tip reporting and revenue capture, so digital payment performance is especially important.\")\n",
    "\n",
    "# Visualization 5: Heatmap day-of-week vs hour\n",
    "heat = df_plot.groupby([\"pickup_day_of_week\",\"pickup_hour\"]).size().reset_index(name=\"trips\")\n",
    "heat_pivot = heat.pivot(index=\"pickup_day_of_week\", columns=\"pickup_hour\", values=\"trips\").fillna(0)\n",
    "fig_heat = center_titles(\n",
    "    px.imshow(\n",
    "        heat_pivot, \n",
    "        title=\"Trips by Day and Hour\", \n",
    "        aspect=\"auto\",\n",
    "        labels={\"x\": \"Pickup Hour\", \"y\": \"Day of Week\", \"color\": \"Number of Trips\"}\n",
    "    )\n",
    ")\n",
    "fig_heat.update_xaxes(dtick=1) # Show every hour on x-axis\n",
    "fig_heat.show()\n",
    "max_idx = heat_pivot.stack().idxmax()\n",
    "max_val = int(heat_pivot.stack().max())\n",
    "print(f\"Interpretation: The busiest time slot is {max_idx[0]} around {int(max_idx[1])}:00 with {max_val:,} trips. Demand is concentrated at specific weekday-hour combinations, useful for staffing and pricing decisions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea59404",
   "metadata": {},
   "source": [
    "### Interactive filters (prototype)\n",
    "Below we provide reusable filter functions that will later be used in the Streamlit app. These functions accept a DataFrame and filter criteria and return a filtered DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters(df, date_min=None, date_max=None, hour_min=0, hour_max=23, payment_types=None):\n",
    "    out = df.copy()\n",
    "    if date_min is not None:\n",
    "        out = out[out['tpep_pickup_datetime'].dt.date >= date_min]\n",
    "    if date_max is not None:\n",
    "        out = out[out['tpep_pickup_datetime'].dt.date <= date_max]\n",
    "    out = out[(out['pickup_hour'] >= hour_min) & (out['pickup_hour'] <= hour_max)]\n",
    "    if payment_types is not None:\n",
    "        out = out[out['payment_type'].isin(payment_types)]\n",
    "    return out\n",
    "\n",
    "# Example usage\n",
    "from datetime import date\n",
    "f = apply_filters(\n",
    "    df_plot, \n",
    "    date_min=date(2024,1,1), \n",
    "    date_max=date(2024,1,31), \n",
    "    hour_min=7, \n",
    "    hour_max=19, \n",
    "    payment_types=[1,2]\n",
    ")\n",
    "print('Filtered rows:', len(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb789635",
   "metadata": {},
   "source": [
    "## Part 3: Streamlit app (`app.py`)\n",
    "The `app.py` file in the repository contains the deployed Streamlit app. It implements the same filters and visualizations as prototypes above and displays required metrics using `st.metric()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b9b8cf",
   "metadata": {},
   "source": [
    "## Part 4: Documentation & Code Quality\n",
    "- Notebook includes modular functions with comments and tests for core behaviors (cleaning and feature engineering).\n",
    "- See `README.md` for setup, deployment, and testing instructions.\n",
    "\n",
    "**AI Assistance:** Deepseek and VSCode Autocompletion provided code suggestions and structure. All code was reviewed and adjusted by the student."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c14631d004031919c98a21c12a12a",
   "metadata": {},
   "source": [
    "## AI Tools Used\n",
    "\n",
    "This assignment used Deepseek and VSCode Autocompletions for code suggestions and outlining. All code was reviewed and finalized by the student (myself).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
